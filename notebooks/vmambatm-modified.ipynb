{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11273377,"sourceType":"datasetVersion","datasetId":7047387},{"sourceId":11273421,"sourceType":"datasetVersion","datasetId":7047422}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Clone repository","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/uol-feps-soc-comp3931-project-2425/individual-project-TianMorley.git\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:43:00.992934Z","iopub.execute_input":"2025-05-04T18:43:00.993390Z","iopub.status.idle":"2025-05-04T18:43:01.130008Z","shell.execute_reply.started":"2025-05-04T18:43:00.993350Z","shell.execute_reply":"2025-05-04T18:43:01.128929Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'individual-project-TianMorley' already exists and is not an empty directory.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Move into working directory","metadata":{}},{"cell_type":"code","source":"%cd individual-project-TianMorley/VMambaTM/VMamba-main","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:43:01.131369Z","iopub.execute_input":"2025-05-04T18:43:01.131635Z","iopub.status.idle":"2025-05-04T18:43:01.138032Z","shell.execute_reply.started":"2025-05-04T18:43:01.131610Z","shell.execute_reply":"2025-05-04T18:43:01.136912Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Install requirements and dependencies","metadata":{}},{"cell_type":"code","source":"!pip install -r requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:43:01.139887Z","iopub.execute_input":"2025-05-04T18:43:01.140192Z","iopub.status.idle":"2025-05-04T18:43:15.843665Z","shell.execute_reply.started":"2025-05-04T18:43:01.140165Z","shell.execute_reply":"2025-05-04T18:43:15.842841Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.20.1+cu121)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.5.1+cu121)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (24.2)\nCollecting triton (from -r requirements.txt (line 5))\n  Downloading triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\nCollecting timm==0.4.12 (from -r requirements.txt (line 6))\n  Downloading timm-0.4.12-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (8.3.4)\nRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (5.2.0)\nCollecting yacs (from -r requirements.txt (line 9))\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (2.5.0)\nCollecting submitit (from -r requirements.txt (line 11))\n  Downloading submitit-1.5.2-py3-none-any.whl.metadata (7.9 kB)\nCollecting tensorboardX (from -r requirements.txt (line 12))\n  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\nCollecting fvcore (from -r requirements.txt (line 13))\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.12.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 2)) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 2)) (11.0.0)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton->-r requirements.txt (line 5)) (75.1.0)\nRequirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->-r requirements.txt (line 7)) (1.2.2)\nRequirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->-r requirements.txt (line 7)) (2.0.0)\nRequirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest->-r requirements.txt (line 7)) (1.5.0)\nRequirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest->-r requirements.txt (line 7)) (2.2.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs->-r requirements.txt (line 9)) (6.0.2)\nRequirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from submitit->-r requirements.txt (line 11)) (3.1.0)\nRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX->-r requirements.txt (line 12)) (3.20.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore->-r requirements.txt (line 13)) (4.67.1)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->-r requirements.txt (line 13)) (0.9.0)\nCollecting iopath>=0.1.7 (from fvcore->-r requirements.txt (line 13))\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from seaborn->-r requirements.txt (line 14)) (2.2.3)\nRequirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.10/dist-packages (from seaborn->-r requirements.txt (line 14)) (3.7.5)\nCollecting portalocker (from iopath>=0.1.7->fvcore->-r requirements.txt (line 13))\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn->-r requirements.txt (line 14)) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn->-r requirements.txt (line 14)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn->-r requirements.txt (line 14)) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn->-r requirements.txt (line 14)) (1.4.7)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn->-r requirements.txt (line 14)) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn->-r requirements.txt (line 14)) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->-r requirements.txt (line 2)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->-r requirements.txt (line 2)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->-r requirements.txt (line 2)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->-r requirements.txt (line 2)) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->-r requirements.txt (line 2)) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->-r requirements.txt (line 2)) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->seaborn->-r requirements.txt (line 14)) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->seaborn->-r requirements.txt (line 14)) (2025.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn->-r requirements.txt (line 14)) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision->-r requirements.txt (line 2)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision->-r requirements.txt (line 2)) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->-r requirements.txt (line 2)) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision->-r requirements.txt (line 2)) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision->-r requirements.txt (line 2)) (2024.2.0)\nDownloading timm-0.4.12-py3-none-any.whl (376 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.4/156.4 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading submitit-1.5.2-py3-none-any.whl (74 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.9/74.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: fvcore, iopath\n  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=36cbfeac0ef58d2ef014b162e4eb827243105728c432fbe099f14a5a165dd21c\n  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=412dcaef6f0577f45b07e13c673350d062814297360fe125a87b90d8105a3dbc\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\nSuccessfully built fvcore iopath\nInstalling collected packages: yacs, triton, submitit, portalocker, iopath, timm, tensorboardX, fvcore\n  Attempting uninstall: timm\n    Found existing installation: timm 1.0.12\n    Uninstalling timm-1.0.12:\n      Successfully uninstalled timm-1.0.12\nSuccessfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.1.1 submitit-1.5.2 tensorboardX-2.6.2.2 timm-0.4.12 triton-3.3.0 yacs-0.1.8\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%cd kernels/selective_scan\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:43:15.844709Z","iopub.execute_input":"2025-05-04T18:43:15.844962Z","iopub.status.idle":"2025-05-04T18:43:15.850523Z","shell.execute_reply.started":"2025-05-04T18:43:15.844938Z","shell.execute_reply":"2025-05-04T18:43:15.849707Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/kernels/selective_scan\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install .\n\n%cd ..\n%cd ..\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:43:15.851270Z","iopub.execute_input":"2025-05-04T18:43:15.851488Z","iopub.status.idle":"2025-05-04T18:44:36.302325Z","shell.execute_reply.started":"2025-05-04T18:43:15.851465Z","shell.execute_reply":"2025-05-04T18:44:36.301491Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/kernels/selective_scan\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from selective_scan==0.0.2) (2.5.1+cu121)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from selective_scan==0.0.2) (24.2)\nRequirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from selective_scan==0.0.2) (1.11.1.3)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from selective_scan==0.0.2) (0.8.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->selective_scan==0.0.2) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->selective_scan==0.0.2) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->selective_scan==0.0.2) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->selective_scan==0.0.2) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->selective_scan==0.0.2) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->selective_scan==0.0.2) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->selective_scan==0.0.2) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->selective_scan==0.0.2) (3.0.2)\nBuilding wheels for collected packages: selective_scan\n  Building wheel for selective_scan (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for selective_scan: filename=selective_scan-0.0.2-cp310-cp310-linux_x86_64.whl size=9260687 sha256=e49f0f1543411c50903743bd6e42bea33ba74786197db8459d7d2a75fb6a3c03\n  Stored in directory: /tmp/pip-ephem-wheel-cache-_42w79xj/wheels/6f/3d/7b/74ba544d5fa7f4bc7d04f0b05525924a07c1ddb9a78300f298\nSuccessfully built selective_scan\nInstalling collected packages: selective_scan\nSuccessfully installed selective_scan-0.0.2\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/kernels\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Move into correct directory and train","metadata":{}},{"cell_type":"code","source":"%cd classification","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:44:36.303259Z","iopub.execute_input":"2025-05-04T18:44:36.303601Z","iopub.status.idle":"2025-05-04T18:44:36.308653Z","shell.execute_reply.started":"2025-05-04T18:44:36.303572Z","shell.execute_reply":"2025-05-04T18:44:36.307967Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!torchrun --nnodes=1 --nproc_per_node=2 --rdzv_id=100 --rdzv_backend=c10d --rdzv_endpoint=\"localhost:29500\" \\\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/main.py \\\n--cfg /kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/configs/vssm/vmambav2_tiny_224.yaml \\\n--batch-size 64 \\\n--data-path /kaggle/input/imgwooflowres/imagewoof2-lowres \\\n--output /kaggle/working/vmamba_checkpoints\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:48:43.096398Z","iopub.execute_input":"2025-05-04T18:48:43.096752Z","iopub.status.idle":"2025-05-04T18:50:17.721073Z","shell.execute_reply.started":"2025-05-04T18:48:43.096716Z","shell.execute_reply":"2025-05-04T18:50:17.719722Z"}},"outputs":[{"name":"stdout","text":"W0504 18:48:44.864000 235 torch/distributed/run.py:793] \nW0504 18:48:44.864000 235 torch/distributed/run.py:793] *****************************************\nW0504 18:48:44.864000 235 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0504 18:48:44.864000 235 torch/distributed/run.py:793] *****************************************\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:74: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=False, oflex=True, backend=None):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:91: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:74: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=False, oflex=True, backend=None):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:91: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:764: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:842: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:764: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:842: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n||fork||=> merge config from /kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/configs/vssm/vmambav2_tiny_224.yaml\nRANK and WORLD_SIZE in environ: 1/2\n||fork||=> merge config from /kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/configs/vssm/vmambav2_tiny_224.yaml\nRANK and WORLD_SIZE in environ: 0/2\n[rank1]:[W504 18:48:49.862956737 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n[rank0]:[W504 18:48:49.888813677 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n/kaggle/working/vmamba_checkpoints/vssm1_tiny_0230/20250504184849/kaggle/working/vmamba_checkpoints/vssm1_tiny_0230/20250504184849\n\n\u001b[32m[2025-05-04 18:48:50 vssm1_tiny_0230]\u001b[0m\u001b[33m(main.py 522)\u001b[0m: INFO Full config saved to /kaggle/working/vmamba_checkpoints/vssm1_tiny_0230/20250504184849/config.json\n\u001b[32m[2025-05-04 18:48:50 vssm1_tiny_0230]\u001b[0m\u001b[33m(main.py 525)\u001b[0m: INFO AMP_ENABLE: true\nAMP_OPT_LEVEL: ''\nAUG:\n  AUTO_AUGMENT: rand-m9-mstd0.5-inc1\n  COLOR_JITTER: 0.4\n  CUTMIX: 1.0\n  CUTMIX_MINMAX: null\n  MIXUP: 0.8\n  MIXUP_MODE: batch\n  MIXUP_PROB: 1.0\n  MIXUP_SWITCH_PROB: 0.5\n  RECOUNT: 1\n  REMODE: pixel\n  REPROB: 0.25\nBASE:\n- ''\nDATA:\n  BATCH_SIZE: 64\n  CACHE_MODE: part\n  DATASET: imagenet\n  DATA_PATH: /kaggle/input/imgwooflowres/imagewoof2-lowres\n  IMG_SIZE: 224\n  INTERPOLATION: bicubic\n  MASK_PATCH_SIZE: 32\n  MASK_RATIO: 0.6\n  NUM_WORKERS: 8\n  PIN_MEMORY: true\n  ZIP_MODE: false\nENABLE_AMP: false\nEVAL_MODE: false\nFUSED_LAYERNORM: false\nMODEL:\n  DROP_PATH_RATE: 0.2\n  DROP_RATE: 0.0\n  LABEL_SMOOTHING: 0.1\n  MMCKPT: false\n  NAME: vssm1_tiny_0230\n  NUM_CLASSES: 1000\n  PRETRAINED: ''\n  RESUME: ''\n  TYPE: vssm\n  VSSM:\n    DEPTHS:\n    - 2\n    - 2\n    - 5\n    - 2\n    DOWNSAMPLE: v3\n    EMBED_DIM: 96\n    GMLP: false\n    IN_CHANS: 3\n    MLP_ACT_LAYER: gelu\n    MLP_DROP_RATE: 0.0\n    MLP_RATIO: 4.0\n    NORM_LAYER: ln2d\n    PATCHEMBED: v2\n    PATCH_NORM: true\n    PATCH_SIZE: 4\n    POSEMBED: false\n    SSM_ACT_LAYER: silu\n    SSM_CONV: 3\n    SSM_CONV_BIAS: false\n    SSM_DROP_RATE: 0.0\n    SSM_DT_RANK: auto\n    SSM_D_STATE: 1\n    SSM_FORWARDTYPE: v05_noz\n    SSM_INIT: v0\n    SSM_LOW_RANK: true\n    SSM_RANK_RATIO: 2.0\n    SSM_RATIO: 2.0\nOUTPUT: /kaggle/working/vmamba_checkpoints/vssm1_tiny_0230/20250504184849\nPRINT_FREQ: 10\nSAVE_FREQ: 10\nSEED: 0\nTAG: '20250504184849'\nTEST:\n  CROP: true\n  SEQUENTIAL: false\n  SHUFFLE: false\nTHROUGHPUT_MODE: false\nTRAIN:\n  ACCUMULATION_STEPS: 1\n  AUTO_RESUME: true\n  BASE_LR: 0.000125\n  CLIP_GRAD: 5.0\n  EPOCHS: 300\n  LAYER_DECAY: 1.0\n  LR_SCHEDULER:\n    DECAY_EPOCHS: 30\n    DECAY_RATE: 0.1\n    GAMMA: 0.1\n    MULTISTEPS: []\n    NAME: cosine\n    WARMUP_PREFIX: true\n  MIN_LR: 1.25e-06\n  MOE:\n    SAVE_MASTER: false\n  OPTIMIZER:\n    BETAS:\n    - 0.9\n    - 0.999\n    EPS: 1.0e-08\n    MOMENTUM: 0.9\n    NAME: adamw\n  START_EPOCH: 0\n  USE_CHECKPOINT: false\n  WARMUP_EPOCHS: 20\n  WARMUP_LR: 1.25e-07\n  WEIGHT_DECAY: 0.05\nTRAINCOST_MODE: false\n\n\u001b[32m[2025-05-04 18:48:50 vssm1_tiny_0230]\u001b[0m\u001b[33m(main.py 526)\u001b[0m: INFO {\"cfg\": \"/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/configs/vssm/vmambav2_tiny_224.yaml\", \"opts\": null, \"batch_size\": 64, \"data_path\": \"/kaggle/input/imgwooflowres/imagewoof2-lowres\", \"zip\": false, \"cache_mode\": \"part\", \"pretrained\": null, \"resume\": null, \"accumulation_steps\": null, \"use_checkpoint\": false, \"disable_amp\": false, \"output\": \"/kaggle/working/vmamba_checkpoints\", \"tag\": \"20250504184849\", \"eval\": false, \"throughput\": false, \"fused_layernorm\": false, \"optim\": null, \"early_stop_patience\": 10, \"early_stop_delta\": 0.0, \"model_ema\": true, \"model_ema_decay\": 0.9999, \"model_ema_force_cpu\": false, \"memory_limit_rate\": -1}\nrank 1 successfully build train dataset\nrank 0 successfully build train dataset\nrank 1 successfully build val dataset\nrank 0 successfully build val dataset\n/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n\u001b[32m[2025-05-04 18:49:00 vssm1_tiny_0230]\u001b[0m\u001b[33m(main.py 139)\u001b[0m: INFO Creating model:vssm/vssm1_tiny_0230\n\u001b[32m[2025-05-04 18:49:01 vssm1_tiny_0230]\u001b[0m\u001b[33m(main.py 174)\u001b[0m: INFO VSSM(\n  (patch_embed): Sequential(\n    (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): Identity()\n    (2): LayerNorm2d((48,), eps=1e-05, elementwise_affine=True)\n    (3): Identity()\n    (4): GELU(approximate='none')\n    (5): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (6): Identity()\n    (7): LayerNorm2d((96,), eps=1e-05, elementwise_affine=True)\n  )\n  (layers): ModuleList(\n    (0): Sequential(\n      (blocks): Sequential(\n        (0): VSSBlock(\n          (norm): LayerNorm2d((96,), eps=1e-05, elementwise_affine=True)\n          (op): SS2D(\n            (out_norm): LayerNorm2d((192,), eps=1e-05, elementwise_affine=True)\n            (in_proj): Linear2d(in_features=96, out_features=192, bias=False)\n            (act): SiLU()\n            (conv2d): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n            (out_act): Identity()\n            (out_proj): Linear2d(in_features=192, out_features=96, bias=False)\n            (dropout): Identity()\n          )\n          (drop_path): timm.DropPath(0.0)\n          (norm2): LayerNorm2d((96,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear2d(in_features=96, out_features=384, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear2d(in_features=384, out_features=96, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (1): VSSBlock(\n          (norm): LayerNorm2d((96,), eps=1e-05, elementwise_affine=True)\n          (op): SS2D(\n            (out_norm): LayerNorm2d((192,), eps=1e-05, elementwise_affine=True)\n            (in_proj): Linear2d(in_features=96, out_features=192, bias=False)\n            (act): SiLU()\n            (conv2d): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n            (out_act): Identity()\n            (out_proj): Linear2d(in_features=192, out_features=96, bias=False)\n            (dropout): Identity()\n          )\n          (drop_path): timm.DropPath(0.019999999552965164)\n          (norm2): LayerNorm2d((96,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear2d(in_features=96, out_features=384, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear2d(in_features=384, out_features=96, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (downsample): Sequential(\n        (0): Identity()\n        (1): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (2): Identity()\n        (3): LayerNorm2d((192,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (1): Sequential(\n      (blocks): Sequential(\n        (0): VSSBlock(\n          (norm): LayerNorm2d((192,), eps=1e-05, elementwise_affine=True)\n          (op): SS2D(\n            (out_norm): LayerNorm2d((384,), eps=1e-05, elementwise_affine=True)\n            (in_proj): Linear2d(in_features=192, out_features=384, bias=False)\n            (act): SiLU()\n            (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (out_act): Identity()\n            (out_proj): Linear2d(in_features=384, out_features=192, bias=False)\n            (dropout): Identity()\n          )\n          (drop_path): timm.DropPath(0.03999999910593033)\n          (norm2): LayerNorm2d((192,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear2d(in_features=192, out_features=768, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear2d(in_features=768, out_features=192, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (1): VSSBlock(\n          (norm): LayerNorm2d((192,), eps=1e-05, elementwise_affine=True)\n          (op): SS2D(\n            (out_norm): LayerNorm2d((384,), eps=1e-05, elementwise_affine=True)\n            (in_proj): Linear2d(in_features=192, out_features=384, bias=False)\n            (act): SiLU()\n            (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (out_act): Identity()\n            (out_proj): Linear2d(in_features=384, out_features=192, bias=False)\n            (dropout): Identity()\n          )\n          (drop_path): timm.DropPath(0.05999999865889549)\n          (norm2): LayerNorm2d((192,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear2d(in_features=192, out_features=768, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear2d(in_features=768, out_features=192, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (downsample): Sequential(\n        (0): Identity()\n        (1): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (2): Identity()\n        (3): LayerNorm2d((384,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (2): Sequential(\n      (blocks): Sequential(\n        (0): VSSBlock(\n          (norm): LayerNorm2d((384,), eps=1e-05, elementwise_affine=True)\n          (op): SS2D(\n            (out_norm): LayerNorm2d((768,), eps=1e-05, elementwise_affine=True)\n            (in_proj): Linear2d(in_features=384, out_features=768, bias=False)\n            (act): SiLU()\n            (conv2d): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n            (out_act): Identity()\n            (out_proj): Linear2d(in_features=768, out_features=384, bias=False)\n            (dropout): Identity()\n          )\n          (drop_path): timm.DropPath(0.07999999821186066)\n          (norm2): LayerNorm2d((384,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear2d(in_features=384, out_features=1536, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear2d(in_features=1536, out_features=384, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (1): VSSBlock(\n          (norm): LayerNorm2d((384,), eps=1e-05, elementwise_affine=True)\n          (op): SS2D(\n            (out_norm): LayerNorm2d((768,), eps=1e-05, elementwise_affine=True)\n            (in_proj): Linear2d(in_features=384, out_features=768, bias=False)\n            (act): SiLU()\n            (conv2d): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n            (out_act): Identity()\n            (out_proj): Linear2d(in_features=768, out_features=384, bias=False)\n            (dropout): Identity()\n          )\n          (drop_path): timm.DropPath(0.10000000894069672)\n          (norm2): LayerNorm2d((384,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear2d(in_features=384, out_features=1536, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear2d(in_features=1536, out_features=384, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (2): VSSBlock(\n          (norm): LayerNorm2d((384,), eps=1e-05, elementwise_affine=True)\n          (op): SS2D(\n            (out_norm): LayerNorm2d((768,), eps=1e-05, elementwise_affine=True)\n            (in_proj): Linear2d(in_features=384, out_features=768, bias=False)\n            (act): SiLU()\n            (conv2d): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n            (out_act): Identity()\n            (out_proj): Linear2d(in_features=768, out_features=384, bias=False)\n            (dropout): Identity()\n          )\n          (drop_path): timm.DropPath(0.12000000476837158)\n          (norm2): LayerNorm2d((384,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear2d(in_features=384, out_features=1536, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear2d(in_features=1536, out_features=384, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (3): VSSBlock(\n          (norm): LayerNorm2d((384,), eps=1e-05, elementwise_affine=True)\n          (op): SS2D(\n            (out_norm): LayerNorm2d((768,), eps=1e-05, elementwise_affine=True)\n            (in_proj): Linear2d(in_features=384, out_features=768, bias=False)\n            (act): SiLU()\n            (conv2d): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n            (out_act): Identity()\n            (out_proj): Linear2d(in_features=768, out_features=384, bias=False)\n            (dropout): Identity()\n          )\n          (drop_path): timm.DropPath(0.14000000059604645)\n          (norm2): LayerNorm2d((384,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear2d(in_features=384, out_features=1536, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear2d(in_features=1536, out_features=384, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (4): VSSBlock(\n          (norm): LayerNorm2d((384,), eps=1e-05, elementwise_affine=True)\n          (op): SS2D(\n            (out_norm): LayerNorm2d((768,), eps=1e-05, elementwise_affine=True)\n            (in_proj): Linear2d(in_features=384, out_features=768, bias=False)\n            (act): SiLU()\n            (conv2d): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n            (out_act): Identity()\n            (out_proj): Linear2d(in_features=768, out_features=384, bias=False)\n            (dropout): Identity()\n          )\n          (drop_path): timm.DropPath(0.1599999964237213)\n          (norm2): LayerNorm2d((384,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear2d(in_features=384, out_features=1536, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear2d(in_features=1536, out_features=384, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (downsample): Sequential(\n        (0): Identity()\n        (1): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (2): Identity()\n        (3): LayerNorm2d((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (3): Sequential(\n      (blocks): Sequential(\n        (0): VSSBlock(\n          (norm): LayerNorm2d((768,), eps=1e-05, elementwise_affine=True)\n          (op): SS2D(\n            (out_norm): LayerNorm2d((1536,), eps=1e-05, elementwise_affine=True)\n            (in_proj): Linear2d(in_features=768, out_features=1536, bias=False)\n            (act): SiLU()\n            (conv2d): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n            (out_act): Identity()\n            (out_proj): Linear2d(in_features=1536, out_features=768, bias=False)\n            (dropout): Identity()\n          )\n          (drop_path): timm.DropPath(0.18000000715255737)\n          (norm2): LayerNorm2d((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear2d(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear2d(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (1): VSSBlock(\n          (norm): LayerNorm2d((768,), eps=1e-05, elementwise_affine=True)\n          (op): SS2D(\n            (out_norm): LayerNorm2d((1536,), eps=1e-05, elementwise_affine=True)\n            (in_proj): Linear2d(in_features=768, out_features=1536, bias=False)\n            (act): SiLU()\n            (conv2d): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n            (out_act): Identity()\n            (out_proj): Linear2d(in_features=1536, out_features=768, bias=False)\n            (dropout): Identity()\n          )\n          (drop_path): timm.DropPath(0.20000000298023224)\n          (norm2): LayerNorm2d((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear2d(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear2d(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (downsample): Identity()\n    )\n  )\n  (classifier): Sequential(\n    (norm): LayerNorm2d((768,), eps=1e-05, elementwise_affine=True)\n    (permute): Identity()\n    (avgpool): AdaptiveAvgPool2d(output_size=1)\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n    (head): Linear(in_features=768, out_features=1000, bias=True)\n  )\n)\n\u001b[32m[2025-05-04 18:49:01 vssm1_tiny_0230]\u001b[0m\u001b[33m(main.py 176)\u001b[0m: INFO number of params: 30705832\ninput params:  u.1 delta.1 A.1 B.1 C.1 D.1 delta_bias.1 \ninput params:  u.3 delta.3 A.3 B.3 C.3 D.3 delta_bias.3 \ninput params:  u.5 delta.5 A.5 B.5 C.5 D.5 delta_bias.5 \ninput params:  u.7 delta.7 A.7 B.7 C.7 D.7 delta_bias.7 \ninput params:  u.9 delta.9 A.9 B.9 C.9 D.9 delta_bias.9 \ninput params:  u.11 delta.11 A.11 B.11 C.11 D.11 delta_bias.11 \ninput params:  u.13 delta.13 A.13 B.13 C.13 D.13 delta_bias.13 \ninput params:  u.15 delta.15 A.15 B.15 C.15 D.15 delta_bias.15 \ninput params:  u.17 delta.17 A.17 B.17 C.17 D.17 delta_bias.17 \ninput params:  u.19 delta.19 A.19 B.19 C.19 D.19 delta_bias.19 \ninput params:  u delta A B C D delta_bias \n\u001b[32m[2025-05-04 18:49:06 vssm1_tiny_0230]\u001b[0m\u001b[33m(main.py 178)\u001b[0m: INFO number of GFLOPs: 4.857742079999999\nUsing EMA with decay = 0.99990000\n\u001b[32m[2025-05-04 18:49:06 vssm1_tiny_0230]\u001b[0m\u001b[33m(optimizer.py 18)\u001b[0m: INFO ==============> building optimizer adamw....................\n\u001b[32m[2025-05-04 18:49:06 vssm1_tiny_0230]\u001b[0m\u001b[33m(optimizer.py 36)\u001b[0m: INFO No weight decay list: ['patch_embed.0.bias', 'patch_embed.2.weight', 'patch_embed.2.bias', 'patch_embed.5.bias', 'patch_embed.7.weight', 'patch_embed.7.bias', 'layers.0.blocks.0.norm.weight', 'layers.0.blocks.0.norm.bias', 'layers.0.blocks.0.op.Ds', 'layers.0.blocks.0.op.out_norm.weight', 'layers.0.blocks.0.op.out_norm.bias', 'layers.0.blocks.0.norm2.weight', 'layers.0.blocks.0.norm2.bias', 'layers.0.blocks.0.mlp.fc1.bias', 'layers.0.blocks.0.mlp.fc2.bias', 'layers.0.blocks.1.norm.weight', 'layers.0.blocks.1.norm.bias', 'layers.0.blocks.1.op.Ds', 'layers.0.blocks.1.op.out_norm.weight', 'layers.0.blocks.1.op.out_norm.bias', 'layers.0.blocks.1.norm2.weight', 'layers.0.blocks.1.norm2.bias', 'layers.0.blocks.1.mlp.fc1.bias', 'layers.0.blocks.1.mlp.fc2.bias', 'layers.0.downsample.1.bias', 'layers.0.downsample.3.weight', 'layers.0.downsample.3.bias', 'layers.1.blocks.0.norm.weight', 'layers.1.blocks.0.norm.bias', 'layers.1.blocks.0.op.Ds', 'layers.1.blocks.0.op.out_norm.weight', 'layers.1.blocks.0.op.out_norm.bias', 'layers.1.blocks.0.norm2.weight', 'layers.1.blocks.0.norm2.bias', 'layers.1.blocks.0.mlp.fc1.bias', 'layers.1.blocks.0.mlp.fc2.bias', 'layers.1.blocks.1.norm.weight', 'layers.1.blocks.1.norm.bias', 'layers.1.blocks.1.op.Ds', 'layers.1.blocks.1.op.out_norm.weight', 'layers.1.blocks.1.op.out_norm.bias', 'layers.1.blocks.1.norm2.weight', 'layers.1.blocks.1.norm2.bias', 'layers.1.blocks.1.mlp.fc1.bias', 'layers.1.blocks.1.mlp.fc2.bias', 'layers.1.downsample.1.bias', 'layers.1.downsample.3.weight', 'layers.1.downsample.3.bias', 'layers.2.blocks.0.norm.weight', 'layers.2.blocks.0.norm.bias', 'layers.2.blocks.0.op.Ds', 'layers.2.blocks.0.op.out_norm.weight', 'layers.2.blocks.0.op.out_norm.bias', 'layers.2.blocks.0.norm2.weight', 'layers.2.blocks.0.norm2.bias', 'layers.2.blocks.0.mlp.fc1.bias', 'layers.2.blocks.0.mlp.fc2.bias', 'layers.2.blocks.1.norm.weight', 'layers.2.blocks.1.norm.bias', 'layers.2.blocks.1.op.Ds', 'layers.2.blocks.1.op.out_norm.weight', 'layers.2.blocks.1.op.out_norm.bias', 'layers.2.blocks.1.norm2.weight', 'layers.2.blocks.1.norm2.bias', 'layers.2.blocks.1.mlp.fc1.bias', 'layers.2.blocks.1.mlp.fc2.bias', 'layers.2.blocks.2.norm.weight', 'layers.2.blocks.2.norm.bias', 'layers.2.blocks.2.op.Ds', 'layers.2.blocks.2.op.out_norm.weight', 'layers.2.blocks.2.op.out_norm.bias', 'layers.2.blocks.2.norm2.weight', 'layers.2.blocks.2.norm2.bias', 'layers.2.blocks.2.mlp.fc1.bias', 'layers.2.blocks.2.mlp.fc2.bias', 'layers.2.blocks.3.norm.weight', 'layers.2.blocks.3.norm.bias', 'layers.2.blocks.3.op.Ds', 'layers.2.blocks.3.op.out_norm.weight', 'layers.2.blocks.3.op.out_norm.bias', 'layers.2.blocks.3.norm2.weight', 'layers.2.blocks.3.norm2.bias', 'layers.2.blocks.3.mlp.fc1.bias', 'layers.2.blocks.3.mlp.fc2.bias', 'layers.2.blocks.4.norm.weight', 'layers.2.blocks.4.norm.bias', 'layers.2.blocks.4.op.Ds', 'layers.2.blocks.4.op.out_norm.weight', 'layers.2.blocks.4.op.out_norm.bias', 'layers.2.blocks.4.norm2.weight', 'layers.2.blocks.4.norm2.bias', 'layers.2.blocks.4.mlp.fc1.bias', 'layers.2.blocks.4.mlp.fc2.bias', 'layers.2.downsample.1.bias', 'layers.2.downsample.3.weight', 'layers.2.downsample.3.bias', 'layers.3.blocks.0.norm.weight', 'layers.3.blocks.0.norm.bias', 'layers.3.blocks.0.op.Ds', 'layers.3.blocks.0.op.out_norm.weight', 'layers.3.blocks.0.op.out_norm.bias', 'layers.3.blocks.0.norm2.weight', 'layers.3.blocks.0.norm2.bias', 'layers.3.blocks.0.mlp.fc1.bias', 'layers.3.blocks.0.mlp.fc2.bias', 'layers.3.blocks.1.norm.weight', 'layers.3.blocks.1.norm.bias', 'layers.3.blocks.1.op.Ds', 'layers.3.blocks.1.op.out_norm.weight', 'layers.3.blocks.1.op.out_norm.bias', 'layers.3.blocks.1.norm2.weight', 'layers.3.blocks.1.norm2.bias', 'layers.3.blocks.1.mlp.fc1.bias', 'layers.3.blocks.1.mlp.fc2.bias', 'classifier.norm.weight', 'classifier.norm.bias', 'classifier.head.bias']\nUsing EMA with decay = 0.99990000\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/utils/utils.py:163: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self._scaler = torch.cuda.amp.GradScaler()\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/utils/utils.py:163: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self._scaler = torch.cuda.amp.GradScaler()\nAll checkpoints founded in /kaggle/working/vmamba_checkpoints/vssm1_tiny_0230/20250504184849: []All checkpoints founded in /kaggle/working/vmamba_checkpoints/vssm1_tiny_0230/20250504184849: []\n\n\u001b[32m[2025-05-04 18:49:06 vssm1_tiny_0230]\u001b[0m\u001b[33m(main.py 227)\u001b[0m: INFO no checkpoint found in /kaggle/working/vmamba_checkpoints/vssm1_tiny_0230/20250504184849, ignoring auto resume\n\u001b[32m[2025-05-04 18:49:06 vssm1_tiny_0230]\u001b[0m\u001b[33m(main.py 261)\u001b[0m: INFO Start training\n/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:74: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=False, oflex=True, backend=None):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:91: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:74: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=False, oflex=True, backend=None):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:91: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:764: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:842: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:764: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:842: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:74: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=False, oflex=True, backend=None):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:91: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:74: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=False, oflex=True, backend=None):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:91: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:764: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:842: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:764: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:842: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:74: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=False, oflex=True, backend=None):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:91: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:74: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=False, oflex=True, backend=None):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:91: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:764: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:842: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:764: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:842: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:74: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=False, oflex=True, backend=None):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:91: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:74: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=False, oflex=True, backend=None):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:91: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:764: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:842: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:764: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:842: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:74: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=False, oflex=True, backend=None):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:91: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:74: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=False, oflex=True, backend=None):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:91: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:764: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:842: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:764: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:842: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:74: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=False, oflex=True, backend=None):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:91: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:74: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=False, oflex=True, backend=None):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:91: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:764: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:842: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:764: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:842: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:74: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=False, oflex=True, backend=None):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:91: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:74: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=False, oflex=True, backend=None):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:91: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:764: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:842: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:764: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:842: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:74: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=False, oflex=True, backend=None):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:91: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:74: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=False, oflex=True, backend=None):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/csms6s.py:91: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:764: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:842: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:764: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/models/mamba2/ssd_combined.py:842: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/main.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=config.AMP_ENABLE):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/main.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=config.AMP_ENABLE):\n/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\ngrad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]\nbucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\ngrad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]\nbucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[32m[2025-05-04 18:50:09 vssm1_tiny_0230]\u001b[0m\u001b[33m(main.py 361)\u001b[0m: INFO Train: [0/300][0/70]\teta 1:12:52 lr 0.000000\t wd 0.0500\ttime 62.4671 (62.4671)\tdata time 40.2157 (40.2157)\tmodel time 0.0000 (0.0000)\tloss 7.0201 (7.0201)\tgrad_norm 11.4938 (11.4938)\tloss_scale 65536.0000 (65536.0000)\tmem 13481MB\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/main.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=config.AMP_ENABLE):\n/kaggle/working/individual-project-TianMorley/VMambaTM/VMamba-main/classification/main.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=config.AMP_ENABLE):\n^C\nW0504 18:50:16.999000 235 torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGINT death signal, shutting down workers\nW0504 18:50:17.000000 235 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 238 closing signal SIGINT\nW0504 18:50:17.000000 235 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 239 closing signal SIGINT\n","output_type":"stream"}],"execution_count":8}]}
